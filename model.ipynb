{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Clinical Records\n",
    "This notebook will take the cleaned data and create, train, and save various machine learning models.\n",
    "\n",
    "All of the models will be implemented using a pipeline so that it can be exported and easily used in a Flask application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Technically here I should only be using `serum_creatinine` and `ejection_fraction` for predictors but I\n",
    "# get average results around 74% accurate.  Including `age` bumps it up a little, which makes sense.\n",
    "# Adding `time` really helps but that's sketchy as discussed in the EDA.\n",
    "\n",
    "all_data = pd.read_csv('./data/cleaned.csv', usecols=['serum_creatinine', 'ejection_fraction', 'age', 'DEATH_EVENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['serum_creatinine'] = 10 ** all_data.serum_creatinine\n",
    "# all_data['ejection_fraction'] = 10 ** all_data.ejection_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draws a learning curve to help visualize model progress and quality over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Splitting the data into train and test sets.  Values are chosen arbitrarily but sensibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = all_data.drop('DEATH_EVENT', axis=1)\n",
    "y = all_data['DEATH_EVENT']\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV # Used by all as well.\n",
    "from sklearn.pipeline import Pipeline # Same^\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# This is shared between all models.\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "SCORING_METRICS = [\n",
    "    'accuracy', # Doesn't work well on imbalanced data, but nice to have around.\n",
    "    'f1',       # Precision+Recall combined (kinda). Perfect for binary classification, which is what we have.\n",
    "    'roc_auc'   # Again good for classification (TPR & FPR), but not good with imbalanced datasets.\n",
    "]\n",
    "MAIN_METRIC = 'f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Pipeline.\n",
    "pl_logreg = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Randomized search space.\n",
    "params_logreg = {\n",
    "    'lr__C': np.linspace(0.0001, 1.0, 30),\n",
    "    'lr__solver': ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']\n",
    "}\n",
    "cv_logreg = RandomizedSearchCV(\n",
    "    pl_logreg,\n",
    "    params_logreg,\n",
    "    cv=kfold, scoring=SCORING_METRICS, refit=MAIN_METRIC, random_state=RANDOM_STATE, n_iter=100, n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_logreg.best_score_\n",
    "# cv_logreg.cv_results_.keys()\n",
    "\n",
    "mean_accuracy = np.mean(cv_logreg.cv_results_['mean_test_accuracy'])\n",
    "print(f'Mean accuracy: {mean_accuracy}')\n",
    "\n",
    "mean_f1 = np.mean(cv_logreg.cv_results_['mean_test_f1'])\n",
    "print(f'Mean F1: {mean_f1}')\n",
    "\n",
    "mean_roc = np.mean(cv_logreg.cv_results_['mean_test_roc_auc'])\n",
    "print(f'Mean ROC: {mean_roc}')\n",
    "\n",
    "plot_learning_curve(cv_logreg.best_estimator_, 'Logistic Regression', X_train, y_train, cv=kfold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pl_forest = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "params_forest = {\n",
    "    'rf__n_estimators': [50, 100, 150, 200],\n",
    "    'rf__max_features': ['auto', 'sqrt'],\n",
    "    'rf__max_depth': [2, 4, 8, 16],\n",
    "    'rf__min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "cv_forest = RandomizedSearchCV(\n",
    "    pl_forest,\n",
    "    params_forest,\n",
    "    cv=kfold, scoring=SCORING_METRICS, refit=MAIN_METRIC, random_state=RANDOM_STATE, n_iter=100, n_jobs=-1\n",
    ")\n",
    "cv_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = np.mean(cv_forest.cv_results_['mean_test_accuracy'])\n",
    "print(f'Mean accuracy: {mean_accuracy}')\n",
    "\n",
    "mean_f1 = np.mean(cv_forest.cv_results_['mean_test_f1'])\n",
    "print(f'Mean F1: {mean_f1}')\n",
    "\n",
    "mean_roc = np.mean(cv_forest.cv_results_['mean_test_roc_auc'])\n",
    "print(f'Mean ROC: {mean_roc}')\n",
    "\n",
    "plot_learning_curve(cv_forest.best_estimator_, 'Random Forest', X_train, y_train, cv=kfold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
